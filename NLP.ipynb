{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b03c37bdc28fe63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c09760a5c92e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### NLTK -> Natural Language Tool-Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T07:59:54.627924Z",
     "start_time": "2024-03-28T07:59:18.676099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/1d/af/4bd17254cdda1d8092460ee5561f013c4ca9c33ecf1aab81b44280327cab/regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/42.0 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 406.8 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 51.2/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in e:\\project\\ai\\nlp\\nlp\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  266.2/268.9 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/268.9 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 92.2/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------  297.0/302.2 kB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 302.2/302.2 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.3 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81dd38028184e7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:00:55.086393Z",
     "start_time": "2024-03-28T08:00:49.675453Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dffd6af95d24351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:01:40.783024Z",
     "start_time": "2024-03-28T08:01:36.104532Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1caf5164821290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:09:43.522840Z",
     "start_time": "2024-03-28T08:09:43.515519Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# It is an unsupervised algorithm that builds a model for abbreviation words, collocations, and words that start sentences; and then uses that model to find sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6958f8095655d0a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:02:01.868321Z",
     "start_time": "2024-03-28T08:02:00.704603Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e73fd87c59694ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:12:38.057909Z",
     "start_time": "2024-03-28T08:12:38.015798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Nltk stop words are widely used words (such as “the,” “a,” “an,” or “in”) that a search engine has been configured to disregard while indexing and retrieving entries. Because they don't necessarily add any value to the meaning of the sentence and makes it bloated for Natural Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b117a36b07a9cc85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:02:44.517525Z",
     "start_time": "2024-03-28T08:02:44.510780Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae721feb53b8f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:03:10.058786Z",
     "start_time": "2024-03-28T08:03:10.051600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a37c7f5b58b2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:04:01.862244Z",
     "start_time": "2024-03-28T08:04:01.823651Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4309f096598324a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:04:28.117768Z",
     "start_time": "2024-03-28T08:04:28.109583Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sent = \"I will walk 500 miles and I would walk 500 more , just to be the man who walks a thousand miles to fall down at your door\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb247871bc6fb3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:04:49.406965Z",
     "start_time": "2024-03-28T08:04:48.822053Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', ',', 'just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd609e63ecff8a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:05:41.278251Z",
     "start_time": "2024-03-28T08:05:41.268693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will walk 500 miles and I would walk 500 more , just to be the man who walks a thousand miles to fall down at your door']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(sent)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ebee97030e16d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:13:34.197225Z",
     "start_time": "2024-03-28T08:13:34.187723Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a192de7ec96c3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:15:32.633004Z",
     "start_time": "2024-03-28T08:15:32.625220Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To remove all these stopwords, we simply iterate over the list of words that we get from word tokenize function and if any words in the sentence match with any of the words in the stopwords list we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "159a1303eb9a8a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:17:11.304583Z",
     "start_time": "2024-03-28T08:17:11.279486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', ',', 'the', 'man', 'walks', 'thousand', 'miles', 'to', 'fall', 'at', 'door']\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in stopwords:\n",
    "        words.remove(word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c600b4a11f12fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:18:48.045372Z",
     "start_time": "2024-03-28T08:18:48.036757Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now that we have eliminated all the words which are not helping, we have the list of words which actually add some value to understanding the meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece72584431d2790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:20:02.172490Z",
     "start_time": "2024-03-28T08:20:02.163947Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# In this list of words the words need to be stemmed so that unessential part of the words are cut down and only pure words remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecec40e40fa00700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:20:17.891464Z",
     "start_time": "2024-03-28T08:20:17.882588Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "754fb5c4b77b7153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:20:34.442451Z",
     "start_time": "2024-03-28T08:20:34.434776Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cd5b0a8d70575a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:24:29.653686Z",
     "start_time": "2024-03-28T08:24:29.644115Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', ',', 'the', 'man', 'walks', 'thousand', 'miles', 'to', 'fall', 'at', 'door']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31d8a4d083ceb9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:23:51.883085Z",
     "start_time": "2024-03-28T08:23:51.772658Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'walk', '500', 'mile', 'i', 'would', 'walk', '500', ',', 'the', 'man', 'walk', 'thousand', 'mile', 'to', 'fall', 'at', 'door']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words] \n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf4ed51b31e9fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:25:31.848612Z",
     "start_time": "2024-03-28T08:25:31.840125Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Another way to stem a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd53b0051c2a49ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:25:44.774837Z",
     "start_time": "2024-03-28T08:25:44.767187Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c132c67eb04e271e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:27:20.797614Z",
     "start_time": "2024-03-28T08:27:20.787128Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'walk', '500', 'mile', 'i', 'would', 'walk', '500', ',', 'the', 'man', 'walk', 'thousand', 'mile', 'to', 'fall', 'at', 'door']\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "snowball_stemming_result = [snowball_stemmer.stem(word) for word in words]\n",
    "print(snowball_stemming_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d75b3083e9f0430f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:30:59.987462Z",
     "start_time": "2024-03-28T08:30:59.979363Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for frequency counting and Parts of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfff01b571a18810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:32:24.417956Z",
     "start_time": "2024-03-28T08:32:24.409689Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c333aa04f1ba09c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:33:35.991849Z",
     "start_time": "2024-03-28T08:33:35.108482Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "712cfc80c2c6155d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:38:05.157405Z",
     "start_time": "2024-03-28T08:38:05.152363Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Counting frequency for stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5a77d6c37ebdd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:40:44.845808Z",
     "start_time": "2024-03-28T08:40:44.838500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'walk': 3, '500': 2, 'mile': 2, 'would': 1, ',': 1, 'the': 1, 'man': 1, 'thousand': 1, 'to': 1, 'fall': 1, 'at': 1, 'door': 1}\n"
     ]
    }
   ],
   "source": [
    "count_dict = {}\n",
    "\n",
    "for word in stemmed_words:\n",
    "    if word in count_dict:\n",
    "        count_dict[word] += 1\n",
    "    else:\n",
    "        count_dict[word] = 1\n",
    "\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700bc6c08d01bfea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:33:47.631627Z",
     "start_time": "2024-03-28T08:33:47.245299Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('walk', 'VBP'), ('500', 'CD'), ('mile', 'NN'), ('i', 'NN'), ('would', 'MD'), ('walk', 'VB'), ('500', 'CD'), (',', ','), ('the', 'DT'), ('man', 'NN'), ('walk', 'NN'), ('thousand', 'VBP'), ('mile', 'NN'), ('to', 'TO'), ('fall', 'VB'), ('at', 'IN'), ('door', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged = pos_tag(stemmed_words)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cf8791c369472c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:37:40.341574Z",
     "start_time": "2024-03-28T08:37:40.334672Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# counting frequency for part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1f71712bba4302b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:36:51.764004Z",
     "start_time": "2024-03-28T08:36:51.756584Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 7, 'VBP': 2, 'CD': 2, 'VB': 2, 'MD': 1, ',': 1, 'DT': 1, 'TO': 1, 'IN': 1})\n"
     ]
    }
   ],
   "source": [
    "count = Counter(tag for _ , tag in pos_tagged)\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
